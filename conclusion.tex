\chapter{Conclusion}

The objective of this thesis was to develop a new class of state estimators for legged robots able to fuse proprioceptive and exteroceptive
sensors. We advocated for the use of tightly-coupled formulations where all measurements and state variables are integrated into a single estimation 
problem. This type of estimator permits a greater accuracy and the estimation of extra parameters by leveraging all sensors correlations.
It also leverages a sound mathematical formulation to extend generic estimators to more diverse and numerous perception sources, toward whole-body estimation.
The accent was put on the necessary modularity of the formulations to make the mathematical developments extendable to new types of sensors. This 
was achieved through the use of Maximum-A-Posteriori estimation, modeled as Factor Graph optimization. All mathematical formulations used extensively the smooth manifold
and Lie group theories that best represent the geometry of the state variables. We also emphasize the generalizability of the developed measurement models. 

\section{Contributions}

We developed a range of measurement models targeted at sensors present on legged platforms. These measurement models were integrated as factors in 
three factor-graph-based estimators.

We implemented a general factor for camera-based object-level pose estimations based on open source libraries. 
First, the \apriltag\ library was used 
to \textbf{obtain poses of fiducial markers} used to augment the scene with unique landmarks. We proposed a new analytical model to estimate the covariance of 
these measurements. We discussed the problem of the orientation ambiguity of these markers and proposed a practical method to alleviate this problem.
The anisotropic nature of the pose uncertainty was highlighted in simulated experiments. 
Second, we used a deep-learning-based framework to obtain the pose of known objects in the scene. A covariance model based on empirical data was obtained through
experiments. We showed that we could fine-tune the model to elements of the scene of interest for legged-robots navigation such as stairs.

We then leverage the pre-integration formulation to include high-frequency sensors such as IMU in the factor graph. We propose 
\textbf{a more systematic formulation of this idea, generalizable to other high-rate proprioceptive sensors}. Within this framework, we proposed a new 
IMU pre-integration algorithm \textbf{based on a compact "delta" Lie-group}.
This approach was theoretically compared to the seminal work on IMU pre-integration, based on composite delta Lie groups. 
We then leveraged the generalized pre-integration method to \textbf{extend the pre-integration to force-torque sensors} present at the end-effectors of
legged robots. 
%
We then proposed a first step toward whole-body estimation, by extending the factor graph approach to also \textbf{estimate centroidal quantities}
(center of mass position and velocity, and angular momentum) by merging centroidal kinematics and force measurements. This makes it possible 
to accurately estimate the centroidal states, despite unavoidable biases in the kinematic model. 


These measurement models were all integrated into the WOLF library \cite{sola2021wolf} to which we contributed, joining our effort with the IRI team, 
toward an open-source release. WOLF provided the necessary modularity to formulate various estimation
problems. It then allowed us to implement three different applications of the proposed theory, which we used to experimentally validate our models.

\bigskip

First, we developed \textbf{a visual-inertial SLAM system based on IMU pre-integration and the \apriltag\ pose measurement model}. This estimator was validated through
a series of experiments conducted at LAAS on the \HRP{2} humanoid robot. We showed that the system provided both: localization and mapping with centimetric precision
for the locomotion of the robot on flat terrain and stairs, and a high-rate, smooth estimation of the base velocity. Both were evaluated against a motion capture 
ground truth. 

Second, we proposed \textbf{a tightly coupled algorithm for simultaneous estimation of the base and centroidal states of the robot}. This estimator combined the pre-integration 
of IMU and force-torque data, leg odometry, and kinematics. We showed that the estimator enables to estimate the bias on CoM kinematics measurements, which is sometimes ignored
by controllers in first approximation. Experimental validation was conducted on simple trajectories performed on the Solo-12 quadruped robot. 

Finally, we implemented \textbf{another object-level visual-inertial SLAM system based on a deep-learning framework for object pose estimation}. This system  used the same 
IMU pre-integration as well as our empirical model of the object pose uncertainty. We showed that the trajectory of the system and objects
in the scene could be recovered and that the IMU contributed to the robustness of the system by alleviating the instability of the pose estimation. We proposed
as a proof of concept to perform visual-inertial SLAM using stairs elements as landmarks, for which we retrained the neural network. This dataset was recorded on the Solo-12 quadruped robot.


\section{Perspectives}
This work laid the foundations of a general factor-graph-based estimation framework for legged robotics. However, a lot of alleys have yet to be explored 
to obtain a usable estimator on any legged platform. Here we present a few of the next projects that we wish to undertake.

\subsection{Short term}
We developed on one hand an object-level visual-inertial system and on the other hand a proprioceptive estimator for the sense of balance. A short-term goal
would be to merge both estimators in one, providing simultaneously a high-rate estimation for control and a non-drifting localization based on SLAM, which should serve also as an input for gait and/or contact planning. 

We began to work in this direction by recording datasets using Solo-12 as an experimental platform. For this experiment, we will concentrate on estimating the 
base state by including inertial, kinematics, and \apriltag\ measurements. 

We also plan to integrate the system in a feedback loop with the current controller of Solo-12. For the moment, instabilities in the solver convergence times prevented
us from obtaining a hard-real-time estimate for the proprioceptive estimator, while the \apriltag\ based SLAM system works in real-time. One of the solutions would 
be a hyperspace search on the numerous options provided by the Ceres solver \cite{ceres-solver}. Another is to search for the most appropriate size of the graph,
that is the frequency of \keyframe\ creation and their number. Marginalization of older states and sparsification procedures should also be investigated.



\subsection{Mid term}

In a second time, we would like to further strengthen the environmental perception of our system. This would be done by developing or integrating a vision system
based on general geometric constraints. Many of the most mature vision-based SLAM systems are based on sparse feature extraction. This will be 
the first venue that we explore. 

Our ideal realization would then be an integrated demo on Solo-12 with an odometry estimator based on kinematics, IMU, and a sparse feature KLT tracking-based 
vision front-end. This system would then easily be extended with either of our object-level SLAM algorithms to provide loop closures and, therefore, a global localization.



\subsection{Longer term}

We will here develop a few ideas and reflections that our work on a general estimator for legged robots brought.

Following the endeavor to develop an estimator fusing as many sources of information, we could turn our attention to a 
\textit{whole-body} state estimation. As we have seen, the standard proprioception sensor set for legged robots is an IMU attached to the robot base
and encoders at the joints (possibly along with joint torque, motor current, and force sensors at the end effector). This set of sensors
is enough to implement proprioceptive odometry of the base but is quite limited when it comes to perceiving finer information about the state of the robot
and its environment. In particular, the kinesthesis sense of artificial-legged systems is far from the subtleties of their biological counterparts, partly
because of the rigid segment assumptions. One solution is to augment the robot with other sensors measuring these flexibilities. Recent solutions have demonstrated
the applicability of such methods by using placing IMUs in each segment of an exoskeleton. One might also imagine adding strain gauges to measure directly 
the deflections of the segments. 

The sense of touch is for the moment also underrepresented in legged robotics. Some teams propose to add
artificial skins that are implemented as strain cells sheets, which provides a higher density haptic feedback than strain-gauges. 
These systems in particular demonstrated a great potential for human-robot interactions. 
A higher-quality sense a touch may also provide richer information about the nature of the contacts between the robot and its environment, be it for locomotion
(slip detection, nature of the terrain, etc.), or for manipulation (object surface analysis, 3D object position estimation). 

On the other end of the spectrum, one could imagine methods targeted toward robots with limited sets of sensors. For instance, Solo-12 is too small to be 
equipped with high-fidelity, multiple-axis strain gauges such as those found on humanoid robots. Such a robot may be equipped with cheap
one-dimensional strain-gauges, which would require new measurement model formulations, with application in centroidal estimation.

With a limited set of sensors, the choice of their nature and placement on the robotic platform is crucial, which is limited by the nature of its initial design. 
On the control side, the concept of co-design is gaining traction. The goal is
to optimize the design of a robot to certain criteria, such as energy efficiency or dynamic capabilities, by exploring the design space guided by 
the simulated control of the platform. A dual concept could be applied to optimize the design of the platform to maximize the efficiency of estimation algorithms.

Our visual-inertial SLAM work based on CosyPose object position is a first step in deriving semantic information from the environment. On this line of research, we may imagine
including more general information about the scene objects by leveraging latent space representations of these deep-learning algorithms. This might enable us to deal with the problem
caused by the symmetries of these objects more properly. Work is also currently conducted on the side of the CosyPose team to better generalize the pose estimation to objects on which the 
model has not been trained. More broadly, the dialog between trained models and optimal estimation has a lot of potentials, both in robotics
and computer vision communities. On one hand, learned factors could be trained with the end goal of improving the result of the estimation algorithm. On the other end,
injecting priors based on physical models of the world, such as the dynamics of legged systems, can be used to improve the results of vision-based algorithms
for dynamic scenes and human pose reconstruction.

% In the context of legged robot locomotion, we showed that systems such as CosyPose were useful to aid the navigation of legged robots with 

Finally, the notion of state feedback may be rethought entirely. Model-based controllers rely on a limited set of physically grounded states to compute the robot control inputs. 
However, some recent learning-based systems do not require this kind of information, relying instead on latent space representations of the robot state learned
in simulation. Some of these algorithms abstract the concepts of robot and environment as a single latent state resulting from a tight coupling of raw proprioceptive and exteroceptive
measurements. While demonstrating impressive results in practice, those representations might have the downside of not being interpretable by a human operator and other systems, 
making the communication and fault detection harder to manage.  


